{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (Support Vector Machines)\n",
    "\n",
    "그래프에 표현된 벡터들 간의 그룹을 구분해주는 선을 출력함.\n",
    "\n",
    "구분선은 각각의 그룹들의 벡터들 중 선과 가장 가까운 벡터들의 거리가 가장 선과 멀 수록 좋은 구분선이다.\n",
    "\n",
    "= Margin (Maximizes the distance to the nearest point)\n",
    "\n",
    "1. 각각의 표현된 벡터들의 정확한 구분이 우선\n",
    "2. 다음이 margin의 최대화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier들이 존재할 때 SVM은 최대로 할 수 있는 한 정확한 답을 내려고 한다. (아웃라이어들을 무시하고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick (kernel parameter in svc)\n",
    "\n",
    "이제까지는 linear한 input들을 위주로 했다면 linear하지 않은 것들을 대상으로 SVM을 쓰기 위해선 무엇을 해야할까? 이를 위해 Kernel Trick이라는 것이 있다. \n",
    "\n",
    "x,y -> x1,x2,x3,x4,.....\n",
    "\n",
    "x,y로 구분되지 않았던 값들을 차원을 늘리므로 구분할 수 있도록 하는 것이다. 그래서 더 높은 차원에서 구분한 구분선을 다시 x,y에 적용시켜 해결할 수 있다.\n",
    "\n",
    "#### kernel의 종류\n",
    "\n",
    "- linear\n",
    "\n",
    "- polynomial\n",
    "\n",
    "- rbf\n",
    "\n",
    "- sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C (c parameter in svc)\n",
    "\n",
    "Smooth decision boundary 와  classifying training points correctly 사이의 조절을 맡음\n",
    "\n",
    "구분선을 일정하게 매끄러운 선으로 그릴시에는 몇개의 포인트가 잘못 분류될 수 있음. 반대로 예민하게 모든 포인트들의 정확한 구분은 매끄러운 선을 그리는 것보다 중요하게 여길 때는 일반적인 구분선이 나오지 않음.\n",
    "\n",
    "C 값이 높을 수록 더 많은 포인트들을 정확하게 구분하여 구불거리는 선이 나옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gamma (gamma parameter in svc)\n",
    "\n",
    "Defines how far the influence of a single training example reaches.\n",
    "\n",
    "얼마나 멀리에 있는 포인트들까지 영향을 미치는지 정해주는 function.\n",
    "\n",
    "감마 값이 낮다면 멀리있는 포인트들까지 관여해서 구분선을 그리고, 감마 값이 높다면 가까이에 있는 포인트들을 주로 영향을 주어서 구분선을 그림. 감마값이 높을 때 가까운 점들이 weight가 높으므로 선이 구불거리게 나올 확률이 높음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting 오버피팅 \n",
    "\n",
    "머신러닝 알고리즘을 사용할 때 항상 주의할 점은 너무 데이터를 overfitting하지는 않았는지 묻는 것이다.\n",
    "\n",
    "감마 값, C값, kernel 을 사용해서 값에 너무 딱 맞는 알고리즘을 찾다보면 결과적으로 다소 과격한 surface를 갖게 되는 현상이 생긴다. 이 것보다 일반적이고 단순한 알고리즘을 찾는 것이 낮다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
